services:
  backend:
    build:
      context: ./backend
    ports:
      - "8000:8000"
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
    volumes:
      - ./backend:/usr/src/app
      - ./backend/logs:/var/log
      - /var/run/docker.sock:/var/run/docker.sock
      - /models:/models
    deploy:
      resources:
        reservations:
          devices:
            - capabilities: [gpu]

  frontend:
    build:
      context: ./frontend
    ports:
      - "7860:7860"
    depends_on:
      - backend

  vllm-service:
    image: vllm/vllm-openai:latest
    runtime: nvidia
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
    volumes:
      - /models:/models
    deploy:
      resources:
        reservations:
          devices:
            - capabilities: [gpu]
    ports:
      - "1370-1380:1370-1380"
    shm_size: 4gb